{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849a491f-a928-423b-a7a4-80ad5a86fae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289814afd1a04ddf9251cd2574fcce9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "pre_train_model = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pre_train_model, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6617e659-0252-463f-84a3-97969211cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "f = open(\"train.json\", \"r\", encoding=\"utf-8\")\n",
    "info = json.load(f)\n",
    "f.close()\n",
    "\n",
    "data = []\n",
    "column_names = ['id', 'utterance', 'emotion']\n",
    "for k, v in info.items() :\n",
    "   data.extend([ [key, i['utterance'], i['emotion']] for s in v for key, i in s.items() ])\n",
    "        # print(key, i['utterance'], i['emotion'])\n",
    "    # break\n",
    "    \n",
    "data_df = pd.DataFrame(data, columns=column_names)\n",
    "data_df[\"emotion\"] = le.fit_transform(data_df.emotion)\n",
    "Xtrain, Xtest = train_test_split(data_df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1f3d0d-458c-4570-8898-3741aa45a722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>那個憨女人有什麼值得送的，正鵬這個人也真是的！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_2</td>\n",
       "      <td>哎喲，老婆子，你怎麼盡講那些不利於團結的話呢！他去送送他的同學也在情理之中嘛！</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_3</td>\n",
       "      <td>爸、媽，我回來啦！</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_4</td>\n",
       "      <td>我怕你喝了迷魂湯，魂被你那個憨同學勾引去了！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5</td>\n",
       "      <td>老婆子，看你講話像個母親說的嗎！你怎麼老是反對他們倆的事呢？</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20609</th>\n",
       "      <td>4186_1</td>\n",
       "      <td>不光是看，我還要說。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20610</th>\n",
       "      <td>4186_2</td>\n",
       "      <td>說什麼？</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20611</th>\n",
       "      <td>4186_3</td>\n",
       "      <td>說我愛你！我要說一百遍一千遍，我愛你！我愛你！我感覺到能在你面前這樣痛痛快快地說出來也是一種幸福。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20612</th>\n",
       "      <td>4187_1</td>\n",
       "      <td>我咋看不出你愛我？</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20613</th>\n",
       "      <td>4187_2</td>\n",
       "      <td>這話我今天第一次說出來，實際我早就是這樣想的了，就是不敢說出來，說了怕你生氣，也怕你不信。我...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20614 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                          utterance  emotion\n",
       "0         1_1                            那個憨女人有什麼值得送的，正鵬這個人也真是的！        1\n",
       "1         1_2            哎喲，老婆子，你怎麼盡講那些不利於團結的話呢！他去送送他的同學也在情理之中嘛！        6\n",
       "2         1_3                                          爸、媽，我回來啦！        4\n",
       "3         1_4                             我怕你喝了迷魂湯，魂被你那個憨同學勾引去了！        1\n",
       "4         1_5                     老婆子，看你講話像個母親說的嗎！你怎麼老是反對他們倆的事呢？        6\n",
       "...       ...                                                ...      ...\n",
       "20609  4186_1                                         不光是看，我還要說。        3\n",
       "20610  4186_2                                               說什麼？        6\n",
       "20611  4186_3  說我愛你！我要說一百遍一千遍，我愛你！我愛你！我感覺到能在你面前這樣痛痛快快地說出來也是一種幸福。        3\n",
       "20612  4187_1                                          我咋看不出你愛我？        3\n",
       "20613  4187_2  這話我今天第一次說出來，實際我早就是這樣想的了，就是不敢說出來，說了怕你生氣，也怕你不信。我...        3\n",
       "\n",
       "[20614 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f1530f-e004-40c1-a385-25698dd70858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# can be up to 512 for BERT\n",
    "max_length = 49\n",
    "batch_size = 20\n",
    "\n",
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "\n",
    "  if (limit > 0):\n",
    "      ds = ds.take(limit)\n",
    "\n",
    "  for index, row in ds.iterrows():\n",
    "    bert_input = convert_example_to_feature(row.utterance)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    # label_list.append(one_hot.loc[index].values)\n",
    "    label_list.append([row.emotion])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "\n",
    "# train dataset\n",
    "ds_train_encoded = encode_examples(Xtrain).shuffle(10000).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(Xtest).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a222da-8ecd-4ef3-9d2a-178d4753deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-6\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 50\n",
    "# model initialization\n",
    "# model = TFBertForSequenceClassification.from_pretrained(pre_train_model, num_labels=32)\n",
    "model = TFBertForSequenceClassification.from_pretrained(pre_train_model, num_labels=32)\n",
    "# model = AutoModel.from_pretrained(pre_train_model, num_labels=32)\n",
    "\n",
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2964dc-cc32-4efd-9836-0bc68b92696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1546/1546 [==============================] - 145s 86ms/step - loss: 1.7858 - accuracy: 0.4504 - val_loss: 1.3581 - val_accuracy: 0.5444\n",
      "Epoch 2/50\n",
      "1546/1546 [==============================] - 129s 84ms/step - loss: 1.2613 - accuracy: 0.5682 - val_loss: 1.2526 - val_accuracy: 0.5617\n",
      "Epoch 3/50\n",
      "1546/1546 [==============================] - 129s 83ms/step - loss: 1.1081 - accuracy: 0.6142 - val_loss: 1.2191 - val_accuracy: 0.5704\n",
      "Epoch 4/50\n",
      "1546/1546 [==============================] - 126s 82ms/step - loss: 0.9925 - accuracy: 0.6553 - val_loss: 1.2131 - val_accuracy: 0.5759\n",
      "Epoch 5/50\n",
      "1546/1546 [==============================] - 125s 81ms/step - loss: 0.8888 - accuracy: 0.6909 - val_loss: 1.2423 - val_accuracy: 0.5797\n",
      "Epoch 6/50\n",
      "1546/1546 [==============================] - 126s 81ms/step - loss: 0.7874 - accuracy: 0.7289 - val_loss: 1.2956 - val_accuracy: 0.5704\n",
      "Epoch 7/50\n",
      "1546/1546 [==============================] - 126s 81ms/step - loss: 0.6921 - accuracy: 0.7633 - val_loss: 1.3469 - val_accuracy: 0.5782\n"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f38e0847-45ef-49a4-9480-346e7ae06065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8_1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8_2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8_3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8_4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8_5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>4188_2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>4188_3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>4188_4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>4188_5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>4188_6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4934 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  emotion\n",
       "0        8_1        4\n",
       "1        8_2        3\n",
       "2        8_3        4\n",
       "3        8_4        4\n",
       "4        8_5        5\n",
       "...      ...      ...\n",
       "4929  4188_2        6\n",
       "4930  4188_3        3\n",
       "4931  4188_4        6\n",
       "4932  4188_5        3\n",
       "4933  4188_6        4\n",
       "\n",
       "[4934 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"test.json\", \"r\", encoding=\"utf-8\")\n",
    "info = json.load(f)\n",
    "f.close()\n",
    "\n",
    "data = []\n",
    "column_names = ['id', 'utterance']\n",
    "for k, v in info.items() :\n",
    "    data.extend([ [key, i['utterance']] for s in v for key, i in s.items() ])\n",
    "    # for s in v :\n",
    "    #     for key, i in s.items():\n",
    "    #         print(i)\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=column_names)\n",
    "ans = []\n",
    "for i in data_df.utterance :\n",
    "    predict_input = tokenizer.encode(i, truncation=True,padding=True,return_tensors=\"tf\")\n",
    "    tf_output = model.predict(predict_input, verbose=0)[0]\n",
    "    tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "    label = tf.argmax(tf_prediction, axis=1)\n",
    "    ans.append(label.numpy()[0])\n",
    "    \n",
    "df_out = pd.DataFrame({\"id\":data_df.id,'emotion': ans})\n",
    "df_out\n",
    "# df_out.to_csv(\"yeelongma.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ce78f3-1365-415a-9804-e114a7272349",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = le.inverse_transform(ans)\n",
    "df_out = pd.DataFrame({\"id\":data_df.id,'emotion': ans})\n",
    "df_out.to_csv(\"yeelongma.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
