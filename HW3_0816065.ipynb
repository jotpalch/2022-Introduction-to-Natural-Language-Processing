{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4-J5jkcatiQ"
   },
   "outputs": [],
   "source": [
    "#Author: Wei-Cheng Chen\n",
    "#Student ID: 0816065\n",
    "#HW ID: Hw3\n",
    "#Due Date: 05/25/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqVxwKpgCe0a",
    "outputId": "08ead65e-c967-4bca-c8fb-8ef93515ee2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jotpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jotpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\jotpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jotpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For debugging\n",
    "import pdb\n",
    "\n",
    "# For checking progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For loading data\n",
    "import pandas as pd\n",
    "\n",
    "# For tokenizaton\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, ne_chunk, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('punkt')\n",
    "\n",
    "# For building n-gram model\n",
    "from collections import Counter, namedtuple\n",
    "import numpy as np\n",
    "\n",
    "# For evaluation \n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# For pos tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "def make_corpus(in_f, out_f):\n",
    "    output = open(out_f, 'w')\n",
    "    wiki = WikiCorpus(in_f)\n",
    "\n",
    "    i = 0\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            print('Processed ' + str(i) + ' articles')\n",
    "    output.close()\n",
    "    print('Processing complete!')\n",
    "    \n",
    "make_corpus(\"enwiki-20220501-pages-meta-history17.xml-p23676046p23716197.bz2\", \"wiki_en_197.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 200003), (2, 38229161), (3, 49712290)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arpa\n",
    "a = arpa.loadf(\"3-gram.arpa\")\n",
    "model = a[0]\n",
    "model.counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b = arpa.loadf(\"4-gram.arpa\")\n",
    "modelf = b[0]\n",
    "modelf.counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Fy2bDYorrKxW"
   },
   "outputs": [],
   "source": [
    "def get_corpus():\n",
    "  df = pd.read_csv('https://raw.githubusercontent.com/yilihsu/NLP110/main/data_tiny.csv')\n",
    "  corpus = df.content.to_list()\n",
    "  return corpus\n",
    "\n",
    "def preprocess(documents):\n",
    "  cleaned_documents = []\n",
    "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~‚Äù'''\n",
    "  for doc in documents:\n",
    "    # Tokenizes the sentence\n",
    "    sents = sent_tokenize(doc)\n",
    "\n",
    "    for sent in sents:\n",
    "      # pdb.set_trace() # delete this line for the final version\n",
    "\n",
    "      # Removes the punctuations [TODO]\n",
    "      for c in punc :\n",
    "        sent = sent.replace(c,\" \")\n",
    "      # Lowers the case\n",
    "      sent = sent.lower() \n",
    "      \n",
    "      cleaned_documents.append(sent)\n",
    "\n",
    "  #print(cleaned_documents[:5])\n",
    "  return cleaned_documents\n",
    "\n",
    "# Compute word frequency\n",
    "def get_vocab(documents):\n",
    "  vocabulary = Counter()\n",
    "\n",
    "  for doc in tqdm(documents):\n",
    "    tokens = word_tokenize(doc)\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "  return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = dict()\n",
    "option = dict()\n",
    "\n",
    "for j in os.listdir(os.getcwd()+\"/test\") :\n",
    "    if \"json\" not in j :\n",
    "        continue\n",
    "    \n",
    "    f = open(os.getcwd()+\"/test/\"+j)\n",
    "    info = json.load(f)\n",
    "    raw = \"<s> \"+info['article']\n",
    "    for i, sent in list(enumerate(raw.split(' _')[:-1])) :\n",
    "        sent = preprocess(sent_tokenize(sent))[-1]\n",
    "        data[info['source']+'_'+str(i)] = sent\n",
    "        option[info['source']+'_'+str(i)] = info['options'][info['source']+'_'+str(i)]\n",
    "    f.close\n",
    "\n",
    "   \n",
    "# for i, sent in list(enumerate(info['article'].split(' _ ')[:-2])) :\n",
    "#     sent = preprocess(sent_tokenize(sent))[-1]\n",
    "#     # print(ne_chunk(pos_tag(word_tokenize(sent))))\n",
    "#     # print(info['source']+'_'+str(i), sent, info['options'][info['source']+'_'+str(i)] )\n",
    "#     data[info['source']+'_'+str(i)] = list(sent, info['options'][info['source']+'_'+str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEDgtK3RxFXt",
    "outputId": "16e72d46-148b-489f-8c44-6c6f6c81a75d"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "raw_documents = get_corpus()\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = get_vocab(raw_documents).most_common(10)\n",
    "print('\\n Before preprocessing:', vocab)\n",
    "\n",
    "# Build vocabulary after preprocessing\n",
    "documents = preprocess(raw_documents)\n",
    "vocab = get_vocab(documents).most_common(10)\n",
    "print('\\n After preprocesing:', vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nsStQG516jC6"
   },
   "outputs": [],
   "source": [
    "class Ngram_model(object):\n",
    "\n",
    "  def __init__(self, documents, N=2):\n",
    "    self.n = N\n",
    "    self.model = self.get_ngram_model(documents)\n",
    "\n",
    "  def get_ngram_model(self, documents):\n",
    "    N = self.n\n",
    "    ngram_model = dict()\n",
    "    full_grams = list()\n",
    "    grams = list()\n",
    "    Word = namedtuple('Word', ['word', 'prob'])\n",
    "\n",
    "    for sentence in documents :\n",
    "      \n",
    "      tokenised = word_tokenize(sentence)\n",
    "\n",
    "      tokenised = ['<s>'] + tokenised + ['<\\s>']\n",
    "\n",
    "      for i in range(len(tokenised)-N+1) :\n",
    "        full_grams.append(tuple(tokenised[i:i+N]))\n",
    "\n",
    "      for i in range(len(tokenised)-N+2) :\n",
    "        grams.append(tuple(tokenised[i:i+N-1]))\n",
    "\n",
    "    full_gram_counter = Counter(full_grams)\n",
    "    gram_counter = Counter(grams)\n",
    "\n",
    "    for key in full_gram_counter:\n",
    "      word = ''.join(key[:N-1])\n",
    "\n",
    "      if word not in ngram_model:\n",
    "        ngram_model.update({word: set()})\n",
    "\n",
    "      next_word_prob = full_gram_counter[key] / gram_counter[key[:N-1]]\n",
    "      w = Word(key[-1], next_word_prob)\n",
    "      ngram_model[word].add(w)\n",
    "\n",
    "    for word, ng in ngram_model.items():\n",
    "      ngram_model[word] = sorted(ng, key=lambda x: x.prob, reverse=True)\n",
    "\n",
    "    return ngram_model\n",
    "\n",
    "\n",
    "  def predict_sent(self, text=None, max_len=30):\n",
    "\n",
    "    N = self.n\n",
    "    backup_tokens = ['<s>']*(N-1)\n",
    "    if not text:\n",
    "      tokens = backup_tokens\n",
    "      output = []\n",
    "\n",
    "    elif type(text)==str:\n",
    "      tokens = backup_tokens + text.split(' ')\n",
    "      tokens = tokens[-(N-1):]\n",
    "      if not self.check_existence(tokens):\n",
    "        return \n",
    "      output = tokens\n",
    "\n",
    "    elif type(text) == list:\n",
    "      tokens = backup_tokens + text\n",
    "      tokens = tokens[-(N-1):]\n",
    "      if not self.check_existence(tokens):\n",
    "        return\n",
    "      output = tokens\n",
    "\n",
    "    else:\n",
    "      print('[Error] the input text must be string or list of string')\n",
    "      return\n",
    "\n",
    "    for i in range(max_len):\n",
    "      possible_words = list(self.model[''.join(tokens)])\n",
    "      probs = [word.prob for word in possible_words]\n",
    "      words = [word.word for word in possible_words]\n",
    "      next_word = np.random.choice(words, 1, p=probs)[0]\n",
    "      tokens = tokens[1:] + [next_word]\n",
    "\n",
    "      if next_word == '<\\\\s>':\n",
    "        break\n",
    "\n",
    "      output.append(next_word)\n",
    "    return ' '.join(output)\n",
    "\n",
    "  def predict_next(self, text=None, top=5):\n",
    "\n",
    "    N = self.n\n",
    "    backup_tokens = ['<s>']*(N-1)\n",
    "    if not text:\n",
    "      tokens = backup_tokens\n",
    "\n",
    "    elif type(text)==str:\n",
    "      tokens = backup_tokens + text.split(' ')\n",
    "      tokens = tokens[-(N-1):]\n",
    "      if not self.check_existence(tokens):\n",
    "        return \n",
    "\n",
    "    elif type(text) == list:\n",
    "      tokens = backup_tokens + text\n",
    "      tokens = tokens[-(N-1):]\n",
    "      if not self.check_existence(tokens):\n",
    "        return\n",
    "    else:\n",
    "      print('[Error] the input text must be string or list of string')\n",
    "\n",
    "    possible_next_words = self.model[''.join(tokens)][:top]\n",
    "    possible_next_words = [(word.word, word.prob) for word in possible_next_words]\n",
    "\n",
    "    return possible_next_words\n",
    "\n",
    "  def check_existence(self, tokens):\n",
    "    if not ''.join(tokens) in self.model.keys():\n",
    "      # print('[Error] the input text {} not in the vocabulary'.format(tokens))\n",
    "      return False\n",
    "    else:\n",
    "      return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h17uKROx-FZG"
   },
   "outputs": [],
   "source": [
    "twogram = Ngram_model(documents, N=2)\n",
    "threegram = Ngram_model(documents, N=3)\n",
    "fourgram = Ngram_model(documents, N=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "opt = [\"A\", \"B\", \"C\", \"D\"]\n",
    "predict = dict()\n",
    "def ClozeTest(id) :\n",
    "    candidate = []\n",
    "    word = word_tokenize(data[id])\n",
    "    pre_four = None\n",
    "    pre_three = None\n",
    "    if len(word) > 2 :\n",
    "        pre_four = fourgram.predict_next(text=word[-3:], top=-1)\n",
    "    if len(word) > 1 :\n",
    "        pre_three = threegram.predict_next(text=word[-2:], top=-1)\n",
    "    if len(word) == 0 :\n",
    "        return None\n",
    "        # predict[id] = random.choice(opt)\n",
    "        # continue\n",
    "    pre_two = twogram.predict_next(text=word[-1], top=-1)\n",
    "    if pre_two != None :\n",
    "        for i in pre_two :\n",
    "            if i[0] in option[id] :\n",
    "                # print(i,opt[option[id].index(i[0])])\n",
    "                candidate.append(i)\n",
    "                break\n",
    "    if pre_three != None :\n",
    "        for i in pre_three :\n",
    "            if i[0] in option[id] :\n",
    "                candidate.append(i)\n",
    "                break\n",
    "    if pre_four != None :\n",
    "        for i in pre_four :\n",
    "            if i[0] in option[id] :\n",
    "                candidate.append(i)\n",
    "                break\n",
    "    if candidate == [] :\n",
    "        # predict[id] = random.choice(opt)\n",
    "        return None\n",
    "    else :\n",
    "        # predict[id] = opt[option[id].index(list(sorted(candidate, key = lambda s: s[1]))[0][0])]\n",
    "        return opt[option[id].index(list(sorted(candidate, key = lambda s: s[1]))[0][0])]\n",
    "\n",
    "# for id in data.keys() :\n",
    "#     ClozeTest(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model_predict = dict()\n",
    "\n",
    "def preprocess_model(documents):\n",
    "  cleaned_documents = []\n",
    "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*~‚Äù'''\n",
    "  for doc in documents:\n",
    "    # Tokenizes the sentence\n",
    "    sents = sent_tokenize(doc)\n",
    "\n",
    "    for sent in sents:\n",
    "      # pdb.set_trace() # delete this line for the final version\n",
    "\n",
    "      # Removes the punctuations [TODO]\n",
    "      for c in punc :\n",
    "        sent = sent.replace(c,\" \")\n",
    "      # Lowers the case\n",
    "      sent = sent.upper() \n",
    "      \n",
    "      cleaned_documents.append(sent)\n",
    "\n",
    "  #print(cleaned_documents[:5])\n",
    "  return cleaned_documents\n",
    "\n",
    "def comb(opt, sent, id):\n",
    "    score = []\n",
    "    opt = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    for o in opt :\n",
    "        sent_c = \" \".join(word_tokenize(sent.replace(\"_\", o.upper() ))) \n",
    "        try :\n",
    "           score.append( model.log_s(sent_c) )\n",
    "        except :\n",
    "           score.append( -1000 )\n",
    "    \n",
    "    ngram = ClozeTest(id)\n",
    "    index = np.argmax(score)\n",
    "    if index == 0 and score[0] == -1000 :  \n",
    "        if ngram == None :\n",
    "            model_predict[id] =  random.choice(opt)\n",
    "        else :\n",
    "            model_predict[id] = ngram\n",
    "\n",
    "    else :\n",
    "        if ngram != opt[index] and ngram != None:\n",
    "            model_predict[id] =   random.choice( [opt[index], ngram] )\n",
    "        else :\n",
    "            model_predict[id] =  opt[index]\n",
    "    \n",
    "    print(model_predict[id], score)\n",
    "\n",
    "\n",
    "\n",
    "proc = 1\n",
    "for j in os.listdir(os.getcwd()+\"/test\") :\n",
    "    if \"json\" not in j :\n",
    "        continue\n",
    "    proc += 1\n",
    "    if proc%100 == 0 :\n",
    "        print(proc)\n",
    "    f = open(os.getcwd()+\"/test/\"+j)\n",
    "    info = json.load(f)\n",
    "    # print(sent_tokenize(info['article']))\n",
    "    sent = list(filter(lambda x: \"_\" in x, sent_tokenize(info['article']) ) )\n",
    "    id = -1\n",
    "    pm_s = preprocess_model(sent)\n",
    "    while id < len(pm_s)-1 :\n",
    "        id += 1\n",
    "        for i in range(1,pm_s[id].count(\"_\")) :\n",
    "            # print(id, info['options'][info['source']+'_'+str(id)], pm_s[id])\n",
    "            comb(info['options'][info['source']+'_'+str(id)], pm_s[id], info['source']+'_'+str(id))\n",
    "            pm_s.insert(id+1, pm_s[id])\n",
    "            id += 1\n",
    "            \n",
    "        if id < len(info['options']) :\n",
    "            comb(info['options'][info['source']+'_'+str(id)], pm_s[id], info['source']+'_'+str(id))\n",
    "        \n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame(list(predict.items()), columns=['id', 'label'])\n",
    "df_predict.to_csv(\"0816065_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoD7wcAJ-PYm",
    "outputId": "5e3380f7-1b02-4800-c8bc-72b52b804c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions of two gram model:\n",
      " [ She said ]\n",
      "('it', 0.1111111111111111)\n",
      "('that', 0.1111111111111111)\n",
      "('she', 0.1111111111111111)\n",
      "('and', 0.05555555555555555)\n",
      "('about', 0.05555555555555555)\n",
      "('today', 0.05555555555555555)\n",
      "('smart', 0.05555555555555555)\n",
      "('come', 0.05555555555555555)\n",
      "('achche', 0.05555555555555555)\n",
      "('sorry', 0.05555555555555555)\n"
     ]
    }
   ],
   "source": [
    "output = threegram.predict_next(text=['she', \"said\"], top=10)\n",
    "print('Next word predictions of two gram model:\\n', \"[ She said ]\")\n",
    "for i in output :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpredict = pd.DataFrame(list(model_predict.items()), columns=['id', 'label'])\n",
    "df_mpredict.to_csv(\"0816065_model_hybra.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBvwvE6q7Vpl",
    "outputId": "1a4af0bb-b652-47ed-e6fc-b8f2d3641e74"
   },
   "source": [
    "output = twogram.predict_sent(max_len=30)\n",
    "print('Generation results of two gram model:', output)\n",
    "nltk.pos_tag(word_tokenize(output))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1_0816065",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
